{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d5e5d885-fd36-4ae1-bf47-1542e5bfa596",
      "metadata": {
        "id": "d5e5d885-fd36-4ae1-bf47-1542e5bfa596"
      },
      "source": [
        "# Assignment #1\n",
        "\n",
        "- In this assignment, your goal is to implement basic neural networks and train them with real-world datasets\n",
        "- Specifically, you should fill the empty code block to successfully train the neural networks\n",
        "- Each ToDo part will be designated with **Your Task** in the markdown, and you need fill the code within the block between *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** and *****End OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7e60dafb-ab21-4a58-8628-28a79959678c",
      "metadata": {
        "id": "7e60dafb-ab21-4a58-8628-28a79959678c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from builtins import range"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!pip install cupy-cuda12x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvrbBUjsRlPJ",
        "outputId": "49b4f58c-6b0e-482d-97c1-8cde2116269d"
      },
      "id": "qvrbBUjsRlPJ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.10/dist-packages (12.2.0)\n",
            "Requirement already satisfied: numpy<1.27,>=1.20 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x) (1.26.4)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x) (0.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### add cupy for gpu acceleration\n",
        "# import cupy as cp"
      ],
      "metadata": {
        "id": "ZYfnxdX2RRIq"
      },
      "id": "ZYfnxdX2RRIq",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "65ddc745-b893-4319-b301-9bdb35a9c423",
      "metadata": {
        "id": "65ddc745-b893-4319-b301-9bdb35a9c423"
      },
      "source": [
        "# Part #0: Dataset Load\n",
        "\n",
        "- In this assignment, we will consider image classification task as our target task\n",
        "- Specifically, we will use CIFAR-10 dataset composed of 50000 32x32 real-world images across 10 different classes\n",
        "- Below codes are written for loading CIFAR-10 dataset from pytorch. If you are hard to understand these, please re-study the basics of pytorch in https://d2l.ai/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6f3bafab-b007-4d4d-9e8c-f262cca1ed04",
      "metadata": {
        "id": "6f3bafab-b007-4d4d-9e8c-f262cca1ed04"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "import statistics\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "plt.rcParams['font.size'] = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c7162dae-c39e-4c7d-bf6c-3b0c1406b62f",
      "metadata": {
        "id": "c7162dae-c39e-4c7d-bf6c-3b0c1406b62f"
      },
      "outputs": [],
      "source": [
        "def cifar10(num_train=None, num_test=None, x_dtype=torch.float32):\n",
        "    \"\"\"\n",
        "    Return the CIFAR10 dataset, automatically downloading it if necessary.\n",
        "    This function can also subsample the dataset.\n",
        "\n",
        "    Inputs:\n",
        "    - num_train: [Optional] How many samples to keep from the training set.\n",
        "      If not provided, then keep the entire training set.\n",
        "    - num_test: [Optional] How many samples to keep from the test set.\n",
        "      If not provided, then keep the entire test set.\n",
        "    - x_dtype: [Optional] Data type of the input image\n",
        "\n",
        "    Returns:\n",
        "    - x_train: `x_dtype` tensor of shape (num_train, 3, 32, 32)\n",
        "    - y_train: int64 tensor of shape (num_train, 3, 32, 32)\n",
        "    - x_test: `x_dtype` tensor of shape (num_test, 3, 32, 32)\n",
        "    - y_test: int64 tensor of shape (num_test, 3, 32, 32)\n",
        "    \"\"\"\n",
        "    download = not os.path.isdir(\"cifar-10-batches-py\")\n",
        "    dset_train = CIFAR10(root=\".\", download=download, train=True)\n",
        "    dset_test = CIFAR10(root=\".\", train=False)\n",
        "\n",
        "    x_train, y_train = dset_train.data[:num_train].astype(\"float\"), np.array(dset_train.targets)[:num_train].astype(np.int64)\n",
        "    x_test, y_test = dset_test.data[:num_test].astype(\"float\"), np.array(dset_test.targets)[:num_test].astype(np.int64)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a08deebb-4035-4e2e-bd40-06d6bca4772d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a08deebb-4035-4e2e-bd40-06d6bca4772d",
        "outputId": "c229adef-5222-43e9-b669-19067917254e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 26.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n"
          ]
        }
      ],
      "source": [
        "x_train, y_train, x_test, y_test = cifar10()\n",
        "\n",
        "print('Training set:', )\n",
        "print('  data shape:', x_train.shape)\n",
        "print('  labels shape: ', y_train.shape)\n",
        "print('Test set:')\n",
        "print('  data shape: ', x_test.shape)\n",
        "print('  labels shape', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc719c70-43c8-4b5d-a56e-826af9252f2b",
      "metadata": {
        "id": "bc719c70-43c8-4b5d-a56e-826af9252f2b"
      },
      "source": [
        "## Visualization of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfd7e28f-b784-4f7c-94a3-0f679178520e",
      "metadata": {
        "id": "dfd7e28f-b784-4f7c-94a3-0f679178520e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "samples_per_class = 12\n",
        "samples = []\n",
        "for y, cls in enumerate(classes):\n",
        "    plt.text(-4, 34 * y + 18, cls, ha='right')\n",
        "    idxs, = (y_train == y).nonzero()\n",
        "    for i in range(samples_per_class):\n",
        "        idx = idxs[random.randrange(idxs.shape[0])].item()\n",
        "        samples.append(x_train[idx].transpose(2,0,1))\n",
        "img = torchvision.utils.make_grid(torch.Tensor(samples), nrow=samples_per_class)\n",
        "plt.imshow(img.permute(1,2,0).to(\"cpu\", torch.uint8).numpy())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e275555-51db-45e2-a5c2-faf64932f8ac",
      "metadata": {
        "id": "9e275555-51db-45e2-a5c2-faf64932f8ac"
      },
      "source": [
        "### Subsampling\n",
        "- When implementing machine learning algorithms, it's usually a good idea to use a small sample of the full dataset. This way your code will run much faster, allowing for more interactive and efficient development. Once you are satisfied that you have correctly implemented the algorithm, you can then rerun with the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f1e40b-0fd7-44a1-a3c4-a583de76f24e",
      "metadata": {
        "id": "c6f1e40b-0fd7-44a1-a3c4-a583de76f24e"
      },
      "outputs": [],
      "source": [
        "num_train = 500\n",
        "num_test = 250\n",
        "\n",
        "x_train, y_train, x_test, y_test = cifar10(num_train, num_test)\n",
        "\n",
        "print('Training set:', )\n",
        "print('  data shape:', x_train.shape)\n",
        "print('  labels shape: ', y_train.shape)\n",
        "print('Test set:')\n",
        "print('  data shape: ', x_test.shape)\n",
        "print('  labels shape', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93cc28e7-fbd8-4917-9360-55224357d9a0",
      "metadata": {
        "id": "93cc28e7-fbd8-4917-9360-55224357d9a0"
      },
      "source": [
        "### Preprocessing\n",
        "- To use these real-world image data with neural networks, common practice is (1) dividing pixel values (0 to 255) into 0 to 1 by dividing with 255 and (2) substracting mean values (or optionally dividing with standard deviation).\n",
        "- This process before running neural network is called pre-process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd024b59-8004-446b-b689-d083390b294a",
      "metadata": {
        "id": "bd024b59-8004-446b-b689-d083390b294a"
      },
      "outputs": [],
      "source": [
        "def get_CIFAR10_data(num_train=49000, num_val=1000, num_test=1000):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare it for the linear classifier.\n",
        "\n",
        "    Inputs:\n",
        "    - num_train: [Optional] Number of training data\n",
        "    - num_val: [Optional] Number of validation data\n",
        "    - num_test: [Optional] Number of test data\n",
        "\n",
        "    Returns:\n",
        "    - x_train: `x_dtype` tensor of shape (num_train, 3, 32, 32)\n",
        "    - y_train: int64 tensor of shape (num_train, 3, 32, 32)\n",
        "    - x_val: `x_dtype` tensor of shape (num_val, 3, 32, 32)\n",
        "    - y_val: int64 tensor of shape (num_val, 3, 32, 32)\n",
        "    - x_test: `x_dtype` tensor of shape (num_test, 3, 32, 32)\n",
        "    - y_test: int64 tensor of shape (num_test, 3, 32, 32)\n",
        "\n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    x_trainval, y_trainval, x_test, y_test = cifar10(num_train + num_val, num_test)\n",
        "\n",
        "    # Subsample the data\n",
        "    mask = list(range(num_train, num_train + num_val))\n",
        "    x_val = x_trainval[mask]\n",
        "    y_val = y_trainval[mask]\n",
        "    mask = list(range(num_train))\n",
        "    x_train = x_trainval[mask]\n",
        "    y_train = y_trainval[mask]\n",
        "\n",
        "    # Preprocessing: divide with 255 and reshape the image data into rows\n",
        "    x_train = np.reshape(x_train / 255, (x_train.shape[0], -1))\n",
        "    x_val = np.reshape(x_val / 255, (x_val.shape[0], -1))\n",
        "    x_test = np.reshape(x_test / 255, (x_test.shape[0], -1))\n",
        "\n",
        "    # Normalize the data: subtract the mean image\n",
        "    mean_image = np.mean(x_train, axis = 0)\n",
        "    x_train -= mean_image\n",
        "    x_val -= mean_image\n",
        "    x_test -= mean_image\n",
        "\n",
        "    # add bias dimension and transform into columns\n",
        "    x_train = np.hstack([x_train, np.ones((x_train.shape[0], 1))])\n",
        "    x_val = np.hstack([x_val, np.ones((x_val.shape[0], 1))])\n",
        "    x_test = np.hstack([x_test, np.ones((x_test.shape[0], 1))])\n",
        "\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43339c8e-a28c-44a8-a4c3-80e5b7496704",
      "metadata": {
        "id": "43339c8e-a28c-44a8-a4c3-80e5b7496704"
      },
      "outputs": [],
      "source": [
        "x_train, y_train, x_val, y_val, x_test, y_test = get_CIFAR10_data()\n",
        "print('Train data shape: ', x_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', x_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', x_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bdb2cd4-5202-4e8d-9aa8-d449f0ff9112",
      "metadata": {
        "id": "3bdb2cd4-5202-4e8d-9aa8-d449f0ff9112"
      },
      "source": [
        "#### Defining dictionary for our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be403a7-eea2-4335-b60d-086d46f84202",
      "metadata": {
        "id": "0be403a7-eea2-4335-b60d-086d46f84202"
      },
      "outputs": [],
      "source": [
        "data = {\"X_train\": x_train, \"y_train\": y_train, \"X_val\": x_val, \"y_val\": y_val, \"X_test\": x_test, \"y_test\": y_test}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fbfe09f-7d69-4e73-b4b6-376bd638bc5b",
      "metadata": {
        "id": "8fbfe09f-7d69-4e73-b4b6-376bd638bc5b"
      },
      "source": [
        "# Part #1: Implementation of Simple Neural Network\n",
        "- Now, let's start to implement neural network.\n",
        "- You will first work on implementing simple 1-layer linear neural network with softmax loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b98f7e4f-a53d-4b04-b7b9-e3d0af62efa7",
      "metadata": {
        "id": "b98f7e4f-a53d-4b04-b7b9-e3d0af62efa7"
      },
      "source": [
        "## Softmax Loss\n",
        "\n",
        "- **Your task #1**: Compute the softmax loss and its gradient using no explicit loops. Store the loss in loss and the gradient in dW. If you are not careful here, it is easy to run into numeric instability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ffc9f32-056a-4f30-a095-99892e0ba766",
      "metadata": {
        "id": "4ffc9f32-056a-4f30-a095-99892e0ba766"
      },
      "outputs": [],
      "source": [
        "def softmax_loss(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Softmax loss function\n",
        "\n",
        "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
        "    of N examples.\n",
        "\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "      that X[i] has label c, where 0 <= c < C.\n",
        "    - reg: (float) weight decay regularization strength\n",
        "\n",
        "    Returns:\n",
        "    - loss: loss as single float\n",
        "    - dW: gradient with respect to weights W; an array of same shape as W\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    # Your task #1\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # Compute scores and normalize them for numerical stability\n",
        "    num_train = X.shape[0]\n",
        "    scores = X.dot(W)  # Shape: (N, C)\n",
        "    scores -= np.max(scores, axis=1, keepdims=True)  # Numerical stability adjustment\n",
        "\n",
        "    # Compute softmax probabilities\n",
        "    exp_scores = np.exp(scores)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # Shape: (N, C)\n",
        "\n",
        "    # Compute the loss\n",
        "    correct_class_probs = probs[np.arange(num_train), y]\n",
        "    loss = -np.sum(np.log(correct_class_probs)) / num_train  # Average cross-entropy loss\n",
        "    loss += 0.5 * reg * np.sum(W * W)  # Add regularization term\n",
        "\n",
        "    # Compute the gradient\n",
        "    dscores = probs\n",
        "    dscores[np.arange(num_train), y] -= 1  # Subtract 1 for correct classes\n",
        "    dscores /= num_train\n",
        "\n",
        "    dW = X.T.dot(dscores)  # Shape: (D, C)\n",
        "    dW += reg * W  # Regularization gradient\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return loss, dW"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed9611f-1248-43bd-9059-574790144eec",
      "metadata": {
        "id": "2ed9611f-1248-43bd-9059-574790144eec"
      },
      "source": [
        "#### Test: Generate a random softmax weight matrix and use it to compute the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb4d551-5aa8-40f4-b653-49e5f058739e",
      "metadata": {
        "id": "0bb4d551-5aa8-40f4-b653-49e5f058739e"
      },
      "outputs": [],
      "source": [
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "loss, grad = softmax_loss(W, x_val, y_val, reg=0.000005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a373360-e28f-4bc0-8240-1a3e8b3b853e",
      "metadata": {
        "id": "7a373360-e28f-4bc0-8240-1a3e8b3b853e"
      },
      "outputs": [],
      "source": [
        "print('loss: %f' % loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5635599d-bb76-43bb-a614-f6ddc213f9a9",
      "metadata": {
        "id": "5635599d-bb76-43bb-a614-f6ddc213f9a9"
      },
      "source": [
        "## Linear Classifier\n",
        "- **Your task #2**: Sample batch_size elements from the training data and their corresponding labels to use in this round of gradient descent. Store the data in ```X_batch``` and their corresponding labels in ```y_batch```; after sampling ```X_batch``` should have shape (batch_size, dim) and ```y_batch``` should have shape (batch_size,). *Hint*: Use np.random.choice to generate indices.\n",
        "- **Your task #3**: Update the weights using the gradient and the learning rate.\n",
        "- **Your task #4**: From the input and learned weights, make a prediction. Store the predicted labels in ```y_pred```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc309045-50be-40c1-8987-3c424041341e",
      "metadata": {
        "id": "bc309045-50be-40c1-8987-3c424041341e"
      },
      "outputs": [],
      "source": [
        "class LinearClassifier(object):\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        X,\n",
        "        y,\n",
        "        learning_rate=1e-3,\n",
        "        reg=1e-5,\n",
        "        num_iters=100,\n",
        "        batch_size=200,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "          means that X[i] has label 0 <= c < C for C classes.\n",
        "        - learning_rate: (float) learning rate for optimization.\n",
        "        - reg: (float) regularization strength.\n",
        "        - num_iters: (integer) number of steps to take when optimizing\n",
        "        - batch_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        num_train, dim = X.shape\n",
        "        num_classes = (\n",
        "            ##### cupy used rather numpy\n",
        "            cp.max(y) + 1\n",
        "        ).item()  # assume y takes values 0...K-1 where K is number of classes\n",
        "        if self.W is None:\n",
        "            # lazily initialize W\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "            ##### cupy used rather numpy\n",
        "            self.W = cp.asarray(self.W)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            # Your task #2\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            # Randomly sample minibatch\n",
        "            batch_indices = np.random.choice(num_train, batch_size, replace=True)  # Sampling with replacement\n",
        "            X_batch = X[batch_indices]\n",
        "            y_batch = y[batch_indices]\n",
        "\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # # Your task #3\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            # Perform parameter update\n",
        "            self.W -= learning_rate * grad\n",
        "\n",
        "\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        # Your task #4\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # Compute class scores\n",
        "        scores = X.dot(self.W)\n",
        "\n",
        "        # Predict the class with the highest score\n",
        "        y_pred = np.argmax(scores, axis=1)\n",
        "\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative.\n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "          data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "        - reg: (float) regularization strength.\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a942afe-c880-44c6-9439-0d27322fd651",
      "metadata": {
        "id": "2a942afe-c880-44c6-9439-0d27322fd651"
      },
      "outputs": [],
      "source": [
        "class Softmax(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        return softmax_loss(self.W, X_batch, y_batch, reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d91aa563-7b43-4546-854e-a51fafe18b51",
      "metadata": {
        "id": "d91aa563-7b43-4546-854e-a51fafe18b51"
      },
      "source": [
        "## Training neural network and optimizing hyperparameters with validation set\n",
        "- **Your task #5**: Use the validation set to tune hyperparameters (regularization strength and learning rate). Also, save the best trained softmax classifer in ```best_softmax```. You should experiment with different ranges for the learning rates and regularization strengths. If you are careful you should be able to get a classification accuracy of over 0.35 on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa43b08-8f38-4512-a050-74a5a638fb74",
      "metadata": {
        "id": "aaa43b08-8f38-4512-a050-74a5a638fb74"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "best_val = -1\n",
        "best_softmax = None\n",
        "\n",
        "# Provided as a reference. You may or may not want to change these hyperparameters\n",
        "learning_rates = np.linspace(1e-3 * 1.5, 1e-2, 10)\n",
        "regularization_strengths = np.linspace(1e-3, 1e-1, 15)\n",
        "\n",
        "# Your task #5\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# Iterate over all combinations of learning rates and regularization strengths\n",
        "X_train = cp.asarray(data[\"X_train\"])\n",
        "y_train = cp.asarray(data[\"y_train\"])\n",
        "X_val = cp.asarray(data[\"X_val\"])\n",
        "y_val = cp.asarray(data[\"y_val\"])\n",
        "for lr in learning_rates:\n",
        "    for reg in regularization_strengths:\n",
        "        # Create a new classifier instance\n",
        "        softmax = Softmax()\n",
        "\n",
        "        # fetch lr and reg to gpu\n",
        "        lr_dev = cp.asarray(lr).item()\n",
        "        reg_dev = cp.asarray(reg).item()\n",
        "\n",
        "        # Train the classifier on the training data\n",
        "        softmax.train(X_train, y_train, learning_rate=lr_dev, reg=reg_dev, num_iters=500)\n",
        "\n",
        "        # Predict on the training and validation sets\n",
        "        train_pred = softmax.predict(X_train)\n",
        "        val_pred = softmax.predict(X_val)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        train_accuracy = cp.mean(train_pred == y_train)\n",
        "        val_accuracy = cp.mean(val_pred == y_val)\n",
        "\n",
        "        # Store the results\n",
        "        results[(lr, reg)] = (train_accuracy, val_accuracy)\n",
        "\n",
        "        # Update the best model if this validation accuracy is the highest\n",
        "        if val_accuracy > best_val:\n",
        "            best_val = val_accuracy\n",
        "            best_softmax = softmax\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "\n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa389512-c84e-4322-adfe-143897df76ad",
      "metadata": {
        "id": "fa389512-c84e-4322-adfe-143897df76ad"
      },
      "source": [
        "Evaluate the best softmax on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "855592ad-f23d-4c0a-8605-90e03f933767",
      "metadata": {
        "id": "855592ad-f23d-4c0a-8605-90e03f933767"
      },
      "outputs": [],
      "source": [
        "#### cupy used rather numpy\n",
        "y_test_pred = best_softmax.predict(cp.asarray(x_test))\n",
        "test_accuracy = cp.mean(cp.asarray(y_test) == y_test_pred)\n",
        "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e9e642-7e5f-47ac-838a-5487533d0ab9",
      "metadata": {
        "id": "16e9e642-7e5f-47ac-838a-5487533d0ab9"
      },
      "source": [
        "# Part #2: Implmentation of deep neural networks\n",
        "- Now, let's start to implement deep neural network, i.e., more than 2 layers with non-linear activation\n",
        "- To this end, we need to implement forward pass (for inference) and backward pass (for gradient calculation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc7f89f-371a-4f32-92be-c57d547cd40c",
      "metadata": {
        "id": "dbc7f89f-371a-4f32-92be-c57d547cd40c"
      },
      "outputs": [],
      "source": [
        "def rel_error(x, y):\n",
        "  \"\"\" returns relative error\n",
        "  Inputs:\n",
        "  - x: inputs 1\n",
        "  - y: inputs 2\n",
        "\n",
        "  Return:\n",
        "  - relative error\n",
        "  \"\"\"\n",
        "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3c180b-caa4-458c-9b94-886f89fa06d3",
      "metadata": {
        "id": "7a3c180b-caa4-458c-9b94-886f89fa06d3"
      },
      "source": [
        "## Forward and Backward Passes for Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebd8ecdd-c903-4af2-9b48-d277fcb94750",
      "metadata": {
        "id": "ebd8ecdd-c903-4af2-9b48-d277fcb94750"
      },
      "source": [
        "### Linear layer: forward\n",
        "- **Your task #6**: Implement the linear forward pass. Store the result in ```out```. You will need to reshape the input into rows.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2fd4423-bbf7-4980-b638-22eb64adb479",
      "metadata": {
        "id": "a2fd4423-bbf7-4980-b638-22eb64adb479"
      },
      "outputs": [],
      "source": [
        "def linear_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a linear (fully-connected) layer.\n",
        "\n",
        "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
        "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
        "    then transform it to an output vector of dimension M.\n",
        "\n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
        "    - w: A numpy array of weights, of shape (D, M)\n",
        "    - b: A numpy array of biases, of shape (M,)\n",
        "\n",
        "    Returns:\n",
        "    - out: output, of shape (N, M)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    # Your task #6\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # Reshape input x to (N, D), where D = d_1 * d_2 * ... * d_k\n",
        "    N = x.shape[0]\n",
        "    x_reshaped = x.reshape(N, -1)  # Flatten all dimensions except the first (batch size)\n",
        "\n",
        "    # Perform the linear transformation\n",
        "    out = x_reshaped.dot(w) + b  # Matrix multiplication plus bias addition\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    cache = (x, w, b)\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f94b07a2-07be-4a27-b1e1-e015a5ca380b",
      "metadata": {
        "id": "f94b07a2-07be-4a27-b1e1-e015a5ca380b"
      },
      "source": [
        "#### Test the linear_forward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1c10256-d58d-4a11-a498-65376a9bb63c",
      "metadata": {
        "id": "f1c10256-d58d-4a11-a498-65376a9bb63c"
      },
      "outputs": [],
      "source": [
        "num_inputs = 2\n",
        "input_shape = (4, 5, 6)\n",
        "output_dim = 3\n",
        "\n",
        "input_size = num_inputs * np.prod(input_shape)\n",
        "weight_size = output_dim * np.prod(input_shape)\n",
        "\n",
        "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
        "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
        "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
        "\n",
        "out, _ = linear_forward(x, w, b)\n",
        "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
        "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
        "\n",
        "# Compare your output with ours. The error should be around e-9 or less.\n",
        "print('Testing linear_forward function:')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0096392-aac3-455f-aea9-5023d0ea9489",
      "metadata": {
        "id": "c0096392-aac3-455f-aea9-5023d0ea9489"
      },
      "source": [
        "### Linear layer: backward\n",
        "- **Your task #7**: Implement the linear backward pass. Store the result in ```dx, dw, db```. You will need to reshape the input into rows.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14504eb-8bac-4abf-96cf-a6c7a5b64c86",
      "metadata": {
        "id": "d14504eb-8bac-4abf-96cf-a6c7a5b64c86"
      },
      "outputs": [],
      "source": [
        "def linear_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a linear layer.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, M)\n",
        "    - cache: Tuple of:\n",
        "      - x: Input data, of shape (N, d_1, ... d_k)\n",
        "      - w: Weights, of shape (D, M)\n",
        "      - b: Biases, of shape (M,)\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
        "    - dw: Gradient with respect to w, of shape (D, M)\n",
        "    - db: Gradient with respect to b, of shape (M,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    dx, dw, db = None, None, None\n",
        "    # Your task #7\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # Reshape x into (N, D) for gradient computation\n",
        "    N = x.shape[0]\n",
        "    x_reshaped = x.reshape(N, -1)  # Shape: (N, D)\n",
        "\n",
        "    # Compute gradients\n",
        "    dw = x_reshaped.T.dot(dout)  # Gradient with respect to w, shape: (D, M)\n",
        "    db = np.sum(dout, axis=0)    # Gradient with respect to b, shape: (M,)\n",
        "    dx_reshaped = dout.dot(w.T)  # Gradient with respect to x_reshaped, shape: (N, D)\n",
        "\n",
        "    # Reshape dx back to the shape of the original input x\n",
        "    dx = dx_reshaped.reshape(*x.shape)\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return dx, dw, db"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e5f9f8b-c994-49a7-97bc-1df4e4b47242",
      "metadata": {
        "id": "6e5f9f8b-c994-49a7-97bc-1df4e4b47242"
      },
      "source": [
        "#### Test the linear_backward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "106d13b7-b9b0-438d-8c01-55b3f5de9b88",
      "metadata": {
        "id": "106d13b7-b9b0-438d-8c01-55b3f5de9b88"
      },
      "outputs": [],
      "source": [
        "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
        "    \"\"\"\n",
        "    Evaluate a numeric gradient for a function that accepts a numpy\n",
        "    array and returns a numpy array.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h\n",
        "        pos = f(x).copy()\n",
        "        x[ix] = oldval - h\n",
        "        neg = f(x).copy()\n",
        "        x[ix] = oldval\n",
        "\n",
        "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
        "        it.iternext()\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c762df7f-5374-41a8-98cf-815f81f6bf09",
      "metadata": {
        "id": "c762df7f-5374-41a8-98cf-815f81f6bf09"
      },
      "outputs": [],
      "source": [
        "np.random.seed(3201)\n",
        "x = np.random.randn(10, 2, 3)\n",
        "w = np.random.randn(6, 5)\n",
        "b = np.random.randn(5)\n",
        "dout = np.random.randn(10, 5)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: linear_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: linear_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: linear_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "_, cache = linear_forward(x, w, b)\n",
        "dx, dw, db = linear_backward(dout, cache)\n",
        "\n",
        "# The error should be around e-10 or less\n",
        "print('Testing linear_backward function:')\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dw error: ', rel_error(dw_num, dw))\n",
        "print('db error: ', rel_error(db_num, db))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280f14f5-4c6d-4cc7-9a6a-40f4a02820b5",
      "metadata": {
        "id": "280f14f5-4c6d-4cc7-9a6a-40f4a02820b5"
      },
      "source": [
        "### ReLU activation: forward\n",
        "- **Your task #8**: Implement the ReLU forward pass. Store the result in ```out```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d895e482-ec21-4424-a25c-95c8ea945901",
      "metadata": {
        "id": "d895e482-ec21-4424-a25c-95c8ea945901"
      },
      "outputs": [],
      "source": [
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    # Your task #8\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # Compute ReLU activation\n",
        "    out = np.maximum(0, x)\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    cache = x\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37330740-739b-4f0a-b1f3-54e6a0bc9753",
      "metadata": {
        "id": "37330740-739b-4f0a-b1f3-54e6a0bc9753"
      },
      "source": [
        "#### Test the relu_forward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b5e5bf4-9f4d-4b06-9964-1471e5af25d7",
      "metadata": {
        "id": "6b5e5bf4-9f4d-4b06-9964-1471e5af25d7"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
        "\n",
        "out, _ = relu_forward(x)\n",
        "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
        "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
        "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
        "\n",
        "# Compare your output with ours. The error should be on the order of e-8\n",
        "print('Testing relu_forward function:')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbe787fb-a5f6-4d2d-b88a-9e3d368758f5",
      "metadata": {
        "id": "bbe787fb-a5f6-4d2d-b88a-9e3d368758f5"
      },
      "source": [
        "### ReLU activation: backward\n",
        "- **Your task #9**: Implement the ReLU backward pass. Store the result in ```dx```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e278aa3-3cb0-427c-b21e-a93cf2b37e20",
      "metadata": {
        "id": "9e278aa3-3cb0-427c-b21e-a93cf2b37e20"
      },
      "outputs": [],
      "source": [
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = None, cache\n",
        "    # Your task #9\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # Compute the gradient of ReLU\n",
        "    dx = dout * (x > 0)\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return dx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b79294c-a38e-474b-bc4c-b1143ffe9e42",
      "metadata": {
        "id": "2b79294c-a38e-474b-bc4c-b1143ffe9e42"
      },
      "source": [
        "#### Test the relu_backward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a3933db-abf5-49bf-bbc8-0f3906dad9e4",
      "metadata": {
        "id": "7a3933db-abf5-49bf-bbc8-0f3906dad9e4"
      },
      "outputs": [],
      "source": [
        "np.random.seed(3201)\n",
        "x = np.random.randn(10, 10)\n",
        "dout = np.random.randn(*x.shape)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
        "\n",
        "_, cache = relu_forward(x)\n",
        "dx = relu_backward(dout, cache)\n",
        "\n",
        "# The error should be on the order of e-12\n",
        "print('Testing relu_backward function:')\n",
        "print('dx error: ', rel_error(dx_num, dx))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f950331-f988-4af5-ab25-981f3e0b22ac",
      "metadata": {
        "id": "3f950331-f988-4af5-ab25-981f3e0b22ac"
      },
      "source": [
        "## Combining linear layer and ReLU activation\n",
        "- There are some common patterns of layers that are frequently used in neural nets. For example, linear layers are frequently followed by a ReLU nonlinearity.\n",
        "- For now take a look at the ```linear_relu_forward``` and `linear_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583db86a-6078-4731-aa17-11d623e36150",
      "metadata": {
        "id": "583db86a-6078-4731-aa17-11d623e36150"
      },
      "outputs": [],
      "source": [
        "def linear_relu_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Convenience layer that perorms an linear transform followed by a ReLU\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input to the linear layer\n",
        "    - w, b: Weights for the linear layer\n",
        "\n",
        "    Returns:\n",
        "    - out: Output from the ReLU\n",
        "    - cache: Object to give to the backward pass\n",
        "    \"\"\"\n",
        "    a, fc_cache = linear_forward(x, w, b)\n",
        "    out, relu_cache = relu_forward(a)\n",
        "    cache = (fc_cache, relu_cache)\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e2fc6b-9777-47a8-8ce2-02393d5e8d6d",
      "metadata": {
        "id": "65e2fc6b-9777-47a8-8ce2-02393d5e8d6d"
      },
      "outputs": [],
      "source": [
        "def linear_relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Backward pass for the linear-relu convenience layer\n",
        "    \"\"\"\n",
        "    fc_cache, relu_cache = cache\n",
        "    da = relu_backward(dout, relu_cache)\n",
        "    dx, dw, db = linear_backward(da, fc_cache)\n",
        "    return dx, dw, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a00329-9b72-4a6c-9a3d-22f4bf4edcf3",
      "metadata": {
        "id": "13a00329-9b72-4a6c-9a3d-22f4bf4edcf3"
      },
      "outputs": [],
      "source": [
        "np.random.seed(3201)\n",
        "x = np.random.randn(2, 3, 4)\n",
        "w = np.random.randn(12, 10)\n",
        "b = np.random.randn(10)\n",
        "dout = np.random.randn(2, 10)\n",
        "\n",
        "out, cache = linear_relu_forward(x, w, b)\n",
        "dx, dw, db = linear_relu_backward(dout, cache)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: linear_relu_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: linear_relu_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: linear_relu_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "# Relative error should be around e-10 or less\n",
        "print('Testing linear_relu_forward and linear_relu_backward:')\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dw error: ', rel_error(dw_num, dw))\n",
        "print('db error: ', rel_error(db_num, db))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "539e89b4-a67e-4c4a-9afb-bba3129b632e",
      "metadata": {
        "id": "539e89b4-a67e-4c4a-9afb-bba3129b632e"
      },
      "source": [
        "### Softmax loss\n",
        "- **Your task #10**: Compute the softmax loss and its gradient w.r.t only input and labels. Store the loss in `loss` and the gradient in `dW`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5834060b-5428-42da-bebe-406cf57b0363",
      "metadata": {
        "id": "5834060b-5428-42da-bebe-406cf57b0363"
      },
      "outputs": [],
      "source": [
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dx = None, None\n",
        "    # Your task #10\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # Number of training examples\n",
        "    N = x.shape[0]\n",
        "\n",
        "    # Shift the logits for numerical stability\n",
        "    shifted_logits = x - np.max(x, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute softmax probabilities\n",
        "    exp_logits = np.exp(shifted_logits)\n",
        "    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute the loss\n",
        "    correct_class_probs = probs[np.arange(N), y]\n",
        "    loss = -np.sum(np.log(correct_class_probs)) / N\n",
        "\n",
        "    # Compute the gradient\n",
        "    dx = probs\n",
        "    dx[np.arange(N), y] -= 1\n",
        "    dx /= N\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return loss, dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dbe8faa-8d9c-4ee5-a483-8040e3c75d0f",
      "metadata": {
        "id": "8dbe8faa-8d9c-4ee5-a483-8040e3c75d0f"
      },
      "outputs": [],
      "source": [
        "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
        "    \"\"\"\n",
        "    a naive implementation of numerical gradient of f at x\n",
        "    - f should be a function that takes a single argument\n",
        "    - x is the point (numpy array) to evaluate the gradient at\n",
        "    \"\"\"\n",
        "\n",
        "    fx = f(x)  # evaluate function value at original point\n",
        "    grad = np.zeros_like(x)\n",
        "    # iterate over all indexes in x\n",
        "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
        "    while not it.finished:\n",
        "\n",
        "        # evaluate function at x+h\n",
        "        ix = it.multi_index\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h  # increment by h\n",
        "        fxph = f(x)  # evalute f(x + h)\n",
        "        x[ix] = oldval - h\n",
        "        fxmh = f(x)  # evaluate f(x - h)\n",
        "        x[ix] = oldval  # restore\n",
        "\n",
        "        # compute the partial derivative with centered formula\n",
        "        grad[ix] = (fxph - fxmh) / (2 * h)  # the slope\n",
        "        if verbose:\n",
        "            print(ix, grad[ix])\n",
        "        it.iternext()  # step to next dimension\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8eaedfd-6d71-4b42-9d01-da9b1167fc92",
      "metadata": {
        "id": "e8eaedfd-6d71-4b42-9d01-da9b1167fc92"
      },
      "outputs": [],
      "source": [
        "np.random.seed(3201)\n",
        "num_classes, num_inputs = 10, 50\n",
        "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
        "y = np.random.randint(num_classes, size=num_inputs)\n",
        "\n",
        "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
        "loss, dx = softmax_loss(x, y)\n",
        "\n",
        "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
        "print('Testing softmax_loss:')\n",
        "print('loss: ', loss)\n",
        "print('dx error: ', rel_error(dx_num, dx))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f024c757-8530-48bc-9c84-06b1222c34d4",
      "metadata": {
        "id": "f024c757-8530-48bc-9c84-06b1222c34d4"
      },
      "source": [
        "## Two-layer neural network\n",
        "- **Your task #11**: Initialize the weights and biases of the two-layer net. Weights should be initialized from a Gaussian centered at 0.0 with standard deviation equal to weight_scale, and biases should be initialized to zero. All weights and biases should be stored in the dictionary `self.params`, with first layer weights and biases using the keys `W1` and `b1` and second layer weights and biases using the keys `W2` and `b2`.\n",
        "- **Your task #12**: Implement the forward pass for the two-layer net, computing the class scores for X and storing them in the `scores` variable.\n",
        "- **Your task #13**: Implement the backward pass for the two-layer net. Store the loss in the `loss` variable and gradients in the `grads` dictionary. Compute data loss using `softmax`, and make sure that `grads[k]` holds the gradients for `self.params[k]`. Don't forget to add L2 regularization. *NOTE*: To ensure that your implementation matches ours and you pass the  automated tests, make sure that your L2 regularization includes a factor of 0.5 to simplify the expression for the gradient.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aef9581-ecbf-4dc2-99a1-f6da8a26b2f8",
      "metadata": {
        "id": "3aef9581-ecbf-4dc2-99a1-f6da8a26b2f8"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNet(object):\n",
        "    \"\"\"\n",
        "    A two-layer fully-connected neural network with ReLU nonlinearity and\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecure should be linear - relu - linear - softmax.\n",
        "\n",
        "    Note that this class does not implement gradient descent; instead, it\n",
        "    will interact with a separate Solver object that is responsible for running\n",
        "    optimization.\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim=3 * 32 * 32 + 1,\n",
        "        hidden_dim=100,\n",
        "        num_classes=10,\n",
        "        weight_scale=1e-3,\n",
        "        reg=0.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        - reg: Scalar giving L2 regularization strength.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        self.reg = reg\n",
        "\n",
        "        # Your task #11\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # Initialize weights and biases for the first layer\n",
        "        #### cupy added\n",
        "        self.params['W1'] = weight_scale * cp.random.randn(input_dim, hidden_dim)\n",
        "        self.params['b1'] = cp.zeros(hidden_dim)\n",
        "\n",
        "        # Initialize weights and biases for the second layer\n",
        "        self.params['W2'] = weight_scale * cp.random.randn(hidden_dim, num_classes)\n",
        "        self.params['b2'] = cp.zeros(num_classes)\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    def loss(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, d_1, ..., d_k)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass and\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        # Your task #12\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # Unpack parameters\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "\n",
        "        # Forward pass: Linear -> ReLU -> Linear\n",
        "        hidden_layer, cache_hidden = linear_forward(X, W1, b1)\n",
        "        relu_out, cache_relu = relu_forward(hidden_layer)\n",
        "        scores, cache_scores = linear_forward(relu_out, W2, b2)\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "\n",
        "        # Your task #13\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # Backward pass\n",
        "\n",
        "        # Compute loss\n",
        "        data_loss, dscores = softmax_loss(scores, y)\n",
        "        reg_loss = 0.5 * self.reg * (cp.sum(W1**2) + cp.sum(W2**2))\n",
        "        loss = data_loss + reg_loss\n",
        "\n",
        "        drelu_out, dW2, db2 = linear_backward(dscores, cache_scores)\n",
        "        dhidden_layer = relu_backward(drelu_out, cache_relu)\n",
        "        _, dW1, db1 = linear_backward(dhidden_layer, cache_hidden)\n",
        "\n",
        "        # Add regularization to gradients\n",
        "        dW2 += self.reg * cp.sqrt(cp.sum(W2 * W2)).item() * 0.5\n",
        "        dW1 += self.reg * cp.sqrt(cp.sum(W1 * W1)).item() * 0.5\n",
        "\n",
        "        # Store gradients\n",
        "        grads = {\n",
        "            'W1': dW1,\n",
        "            'b1': db1,\n",
        "            'W2': dW2,\n",
        "            'b2': db2,\n",
        "        }\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        return loss, grads"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = cp.asarray([1, 2, 3])\n",
        "print(type(cp.sqrt(cp.sum(a * a))))"
      ],
      "metadata": {
        "id": "kGHX10_QB3fN"
      },
      "id": "kGHX10_QB3fN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a6c9be-7b58-43c0-b30d-d2aa6a16e6d0",
      "metadata": {
        "id": "a8a6c9be-7b58-43c0-b30d-d2aa6a16e6d0"
      },
      "outputs": [],
      "source": [
        "np.random.seed(3201)\n",
        "N, D, H, C = 3, 5, 50, 7\n",
        "X = np.random.randn(N, D)\n",
        "y = np.random.randint(C, size=N)\n",
        "\n",
        "std = 1e-3\n",
        "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
        "\n",
        "print('Testing initialization ... ')\n",
        "W1_std = abs(model.params['W1'].std() - std)\n",
        "b1 = model.params['b1']\n",
        "W2_std = abs(model.params['W2'].std() - std)\n",
        "b2 = model.params['b2']\n",
        "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
        "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
        "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
        "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
        "\n",
        "print('Testing test-time forward pass ... ')\n",
        "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
        "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
        "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
        "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
        "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
        "scores = model.loss(X)\n",
        "correct_scores = np.asarray(\n",
        "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
        "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
        "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
        "scores_diff = np.abs(scores - correct_scores).sum()\n",
        "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
        "\n",
        "print('Testing training loss (no regularization)')\n",
        "y = np.asarray([0, 5, 1])\n",
        "loss, grads = model.loss(X, y)\n",
        "correct_loss = 3.4702243556\n",
        "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
        "\n",
        "model.reg = 1.0\n",
        "loss, grads = model.loss(X, y)\n",
        "correct_loss = 26.5948426952\n",
        "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
        "\n",
        "# Errors should be around e-7 or less\n",
        "for reg in [0.0, 0.7]:\n",
        "  print('Running numeric gradient check with reg = ', reg)\n",
        "  model.reg = reg\n",
        "  loss, grads = model.loss(X, y)\n",
        "\n",
        "  for name in sorted(grads):\n",
        "    f = lambda _: model.loss(X, y)[0]\n",
        "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
        "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f638fb9d-0343-4fd0-82fa-cf9875ee328f",
      "metadata": {
        "id": "f638fb9d-0343-4fd0-82fa-cf9875ee328f"
      },
      "source": [
        "## Solver\n",
        "- Now, you will train the defined neural network using **Solver**. Solver-like function is a widely used nowadays (e.g., huggingface Trainser: https://huggingface.co/docs/transformers/main_classes/trainer).\n",
        "- **Caution**: Your task is NOT implemting solver; just use the prodived solver below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc91556-0731-41fa-86e7-756b10646023",
      "metadata": {
        "id": "5cc91556-0731-41fa-86e7-756b10646023"
      },
      "outputs": [],
      "source": [
        "class Solver(object):\n",
        "    \"\"\"\n",
        "    A Solver encapsulates all the logic necessary for training classification\n",
        "    models. The Solver performs stochastic gradient descent using different\n",
        "    update rules defined in optim.py.\n",
        "\n",
        "    The solver accepts both training and validation data and labels so it can\n",
        "    periodically check classification accuracy on both training and validation\n",
        "    data to watch out for overfitting.\n",
        "\n",
        "    To train a model, you will first construct a Solver instance, passing the\n",
        "    model, dataset, and various options (learning rate, batch size, etc) to the\n",
        "    constructor. You will then call the train() method to run the optimization\n",
        "    procedure and train the model.\n",
        "\n",
        "    After the train() method returns, model.params will contain the parameters\n",
        "    that performed best on the validation set over the course of training.\n",
        "    In addition, the instance variable solver.loss_history will contain a list\n",
        "    of all losses encountered during training and the instance variables\n",
        "    solver.train_acc_history and solver.val_acc_history will be lists of the\n",
        "    accuracies of the model on the training and validation set at each epoch.\n",
        "\n",
        "    Example usage might look something like this:\n",
        "\n",
        "    data = {\n",
        "      'X_train': # training data\n",
        "      'y_train': # training labels\n",
        "      'X_val': # validation data\n",
        "      'y_val': # validation labels\n",
        "    }\n",
        "    model = MyAwesomeModel(hidden_size=100, reg=10)\n",
        "    solver = Solver(model, data,\n",
        "                    update_rule='sgd',\n",
        "                    optim_config={\n",
        "                      'learning_rate': 1e-3,\n",
        "                    },\n",
        "                    lr_decay=0.95,\n",
        "                    num_epochs=10, batch_size=100,\n",
        "                    print_every=100)\n",
        "    solver.train()\n",
        "\n",
        "\n",
        "    A Solver works on a model object that must conform to the following API:\n",
        "\n",
        "    - model.params must be a dictionary mapping string parameter names to numpy\n",
        "      arrays containing parameter values.\n",
        "\n",
        "    - model.loss(X, y) must be a function that computes training-time loss and\n",
        "      gradients, and test-time classification scores, with the following inputs\n",
        "      and outputs:\n",
        "\n",
        "      Inputs:\n",
        "      - X: Array giving a minibatch of input data of shape (N, d_1, ..., d_k)\n",
        "      - y: Array of labels, of shape (N,) giving labels for X where y[i] is the\n",
        "        label for X[i].\n",
        "\n",
        "      Returns:\n",
        "      If y is None, run a test-time forward pass and return:\n",
        "      - scores: Array of shape (N, C) giving classification scores for X where\n",
        "        scores[i, c] gives the score of class c for X[i].\n",
        "\n",
        "      If y is not None, run a training time forward and backward pass and\n",
        "      return a tuple of:\n",
        "      - loss: Scalar giving the loss\n",
        "      - grads: Dictionary with the same keys as self.params mapping parameter\n",
        "        names to gradients of the loss with respect to those parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, data, **kwargs):\n",
        "        \"\"\"\n",
        "        Construct a new Solver instance.\n",
        "\n",
        "        Required arguments:\n",
        "        - model: A model object conforming to the API described above\n",
        "        - data: A dictionary of training and validation data containing:\n",
        "          'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "          'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "          'y_train': Array, shape (N_train,) of labels for training images\n",
        "          'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "        Optional arguments:\n",
        "        - update_rule: A string giving the name of an update rule in optim.py.\n",
        "          Default is 'sgd'.\n",
        "        - optim_config: A dictionary containing hyperparameters that will be\n",
        "          passed to the chosen update rule. Each update rule requires different\n",
        "          hyperparameters (see optim.py) but all update rules require a\n",
        "          'learning_rate' parameter so that should always be present.\n",
        "        - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "          learning rate is multiplied by this value.\n",
        "        - batch_size: Size of minibatches used to compute loss and gradient\n",
        "          during training.\n",
        "        - num_epochs: The number of epochs to run for during training.\n",
        "        - print_every: Integer; training losses will be printed every\n",
        "          print_every iterations.\n",
        "        - verbose: Boolean; if set to false then no output will be printed\n",
        "          during training.\n",
        "        - num_train_samples: Number of training samples used to check training\n",
        "          accuracy; default is 1000; set to None to use entire training set.\n",
        "        - num_val_samples: Number of validation samples to use to check val\n",
        "          accuracy; default is None, which uses the entire validation set.\n",
        "        - checkpoint_name: If not None, then save model checkpoints here every\n",
        "          epoch.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.X_train = data[\"X_train\"]\n",
        "        self.y_train = data[\"y_train\"]\n",
        "        self.X_val = data[\"X_val\"]\n",
        "        self.y_val = data[\"y_val\"]\n",
        "\n",
        "        # Unpack keyword arguments\n",
        "        self.update_rule = kwargs.pop(\"update_rule\", \"sgd\")\n",
        "        self.optim_config = kwargs.pop(\"optim_config\", {})\n",
        "        self.lr_decay = kwargs.pop(\"lr_decay\", 1.0)\n",
        "        self.batch_size = kwargs.pop(\"batch_size\", 100)\n",
        "        self.num_epochs = kwargs.pop(\"num_epochs\", 10)\n",
        "        self.num_train_samples = kwargs.pop(\"num_train_samples\", 1000)\n",
        "        self.num_val_samples = kwargs.pop(\"num_val_samples\", None)\n",
        "\n",
        "        self.checkpoint_name = kwargs.pop(\"checkpoint_name\", None)\n",
        "        self.print_every = kwargs.pop(\"print_every\", 10)\n",
        "        self.verbose = kwargs.pop(\"verbose\", True)\n",
        "\n",
        "        # Throw an error if there are extra keyword arguments\n",
        "        if len(kwargs) > 0:\n",
        "            extra = \", \".join('\"%s\"' % k for k in list(kwargs.keys()))\n",
        "            raise ValueError(\"Unrecognized arguments %s\" % extra)\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        Set up some book-keeping variables for optimization. Don't call this\n",
        "        manually.\n",
        "        \"\"\"\n",
        "        # Set up some variables for book-keeping\n",
        "        self.epoch = 0\n",
        "        self.best_val_acc = 0\n",
        "        self.best_params = {}\n",
        "        self.loss_history = []\n",
        "        self.train_acc_history = []\n",
        "        self.val_acc_history = []\n",
        "\n",
        "        # Make a deep copy of the optim_config for each parameter\n",
        "        self.optim_configs = {}\n",
        "        for p in self.model.params:\n",
        "            d = {k: v for k, v in self.optim_config.items()}\n",
        "            self.optim_configs[p] = d\n",
        "\n",
        "    def _step(self):\n",
        "        \"\"\"\n",
        "        Make a single gradient update. This is called by train() and should not\n",
        "        be called manually.\n",
        "        \"\"\"\n",
        "        # Make a minibatch of training data\n",
        "        num_train = self.X_train.shape[0]\n",
        "        batch_mask = np.random.choice(num_train, self.batch_size)\n",
        "        X_batch = self.X_train[batch_mask]\n",
        "        y_batch = self.y_train[batch_mask]\n",
        "\n",
        "        # Compute loss and gradient\n",
        "        loss, grads = self.model.loss(X_batch, y_batch)\n",
        "        self.loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in self.model.params.items():\n",
        "            dw = grads[p]\n",
        "            config = self.optim_configs[p]\n",
        "            next_w, next_config = sgd(w, dw, config)\n",
        "            self.model.params[p] = next_w\n",
        "            self.optim_configs[p] = next_config\n",
        "\n",
        "    def _save_checkpoint(self):\n",
        "        if self.checkpoint_name is None:\n",
        "            return\n",
        "        checkpoint = {\n",
        "            \"model\": self.model,\n",
        "            \"update_rule\": self.update_rule,\n",
        "            \"lr_decay\": self.lr_decay,\n",
        "            \"optim_config\": self.optim_config,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"num_train_samples\": self.num_train_samples,\n",
        "            \"num_val_samples\": self.num_val_samples,\n",
        "            \"epoch\": self.epoch,\n",
        "            \"loss_history\": self.loss_history,\n",
        "            \"train_acc_history\": self.train_acc_history,\n",
        "            \"val_acc_history\": self.val_acc_history,\n",
        "        }\n",
        "        filename = \"%s_epoch_%d.pkl\" % (self.checkpoint_name, self.epoch)\n",
        "        if self.verbose:\n",
        "            print('Saving checkpoint to \"%s\"' % filename)\n",
        "        with open(filename, \"wb\") as f:\n",
        "            pickle.dump(checkpoint, f)\n",
        "\n",
        "    def check_accuracy(self, X, y, num_samples=None, batch_size=100):\n",
        "        \"\"\"\n",
        "        Check accuracy of the model on the provided data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "        - y: Array of labels, of shape (N,)\n",
        "        - num_samples: If not None, subsample the data and only test the model\n",
        "          on num_samples datapoints.\n",
        "        - batch_size: Split X and y into batches of this size to avoid using\n",
        "          too much memory.\n",
        "\n",
        "        Returns:\n",
        "        - acc: Scalar giving the fraction of instances that were correctly\n",
        "          classified by the model.\n",
        "        \"\"\"\n",
        "\n",
        "        # Maybe subsample the data\n",
        "        N = X.shape[0]\n",
        "        if num_samples is not None and N > num_samples:\n",
        "            mask = np.random.choice(N, num_samples)\n",
        "            N = num_samples\n",
        "            X = X[mask]\n",
        "            y = y[mask]\n",
        "\n",
        "        # Compute predictions in batches\n",
        "        num_batches = N // batch_size\n",
        "        if N % batch_size != 0:\n",
        "            num_batches += 1\n",
        "        y_pred = []\n",
        "        for i in range(num_batches):\n",
        "            start = i * batch_size\n",
        "            end = (i + 1) * batch_size\n",
        "            scores = self.model.loss(X[start:end])\n",
        "            y_pred.append(np.argmax(scores, axis=1))\n",
        "        y_pred = np.hstack(y_pred)\n",
        "        acc = np.mean(y_pred == y)\n",
        "\n",
        "        return acc\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Run optimization to train the model.\n",
        "        \"\"\"\n",
        "        num_train = self.X_train.shape[0]\n",
        "        iterations_per_epoch = max(num_train // self.batch_size, 1)\n",
        "        num_iterations = self.num_epochs * iterations_per_epoch\n",
        "\n",
        "        for t in range(num_iterations):\n",
        "            self._step()\n",
        "\n",
        "            # Maybe print training loss\n",
        "            if self.verbose and t % self.print_every == 0:\n",
        "                print(\n",
        "                    \"(Iteration %d / %d) loss: %f\"\n",
        "                    % (t + 1, num_iterations, self.loss_history[-1])\n",
        "                )\n",
        "\n",
        "            # At the end of every epoch, increment the epoch counter and decay\n",
        "            # the learning rate.\n",
        "            epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "            if epoch_end:\n",
        "                self.epoch += 1\n",
        "                for k in self.optim_configs:\n",
        "                    self.optim_configs[k][\"learning_rate\"] *= self.lr_decay\n",
        "\n",
        "            # Check train and val accuracy on the first iteration, the last\n",
        "            # iteration, and at the end of each epoch.\n",
        "            first_it = t == 0\n",
        "            last_it = t == num_iterations - 1\n",
        "            if first_it or last_it or epoch_end:\n",
        "                train_acc = self.check_accuracy(\n",
        "                    self.X_train, self.y_train, num_samples=self.num_train_samples\n",
        "                )\n",
        "                val_acc = self.check_accuracy(\n",
        "                    self.X_val, self.y_val, num_samples=self.num_val_samples\n",
        "                )\n",
        "                self.train_acc_history.append(train_acc)\n",
        "                self.val_acc_history.append(val_acc)\n",
        "                self._save_checkpoint()\n",
        "\n",
        "                if self.verbose:\n",
        "                    print(\n",
        "                        \"(Epoch %d / %d) train acc: %f; val_acc: %f\"\n",
        "                        % (self.epoch, self.num_epochs, train_acc, val_acc)\n",
        "                    )\n",
        "\n",
        "                # Keep track of the best model\n",
        "                if val_acc > self.best_val_acc:\n",
        "                    self.best_val_acc = val_acc\n",
        "                    self.best_params = {}\n",
        "                    for k, v in self.model.params.items():\n",
        "                        self.best_params[k] = v.copy()\n",
        "\n",
        "        # At the end of training swap the best params into the model\n",
        "        self.model.params = self.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0aea9c5-8781-4ecc-b1d0-4c065b413d4e",
      "metadata": {
        "id": "f0aea9c5-8781-4ecc-b1d0-4c065b413d4e"
      },
      "source": [
        "### Stochastic Gradient Descent (SGD)\n",
        "- **Your task #14**: Implement the vanilla stochastic gradient descent update formula, which will be used with Solver above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8571c373-c3b1-4e1c-afca-4b465bc185db",
      "metadata": {
        "id": "8571c373-c3b1-4e1c-afca-4b465bc185db"
      },
      "outputs": [],
      "source": [
        "def sgd(w, dw, config=None):\n",
        "    \"\"\"\n",
        "    Performs vanilla stochastic gradient descent.\n",
        "\n",
        "    config format:\n",
        "    - learning_rate: Scalar learning rate.\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = {}\n",
        "    config.setdefault(\"learning_rate\", 1e-2)\n",
        "\n",
        "    # Your task #14\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # Update weights using the gradient and learning rate\n",
        "    w -= config[\"learning_rate\"] * dw\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return w, config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e235f362-67da-4afc-880d-7aaf3147f3b4",
      "metadata": {
        "id": "e235f362-67da-4afc-880d-7aaf3147f3b4"
      },
      "source": [
        "- **Your task #15**: Use a Solver instance to train a TwoLayerNet that achieves about 36% accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcb45bfd-cfaf-4edb-95a1-8371cdd17327",
      "metadata": {
        "id": "fcb45bfd-cfaf-4edb-95a1-8371cdd17327"
      },
      "outputs": [],
      "source": [
        "input_size = 32 * 32 * 3 + 1 # 1 for bias\n",
        "hidden_size = 50\n",
        "num_classes = 10\n",
        "model = TwoLayerNet(input_size, hidden_size, num_classes)\n",
        "solver = None\n",
        "\n",
        "# Your task #15\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# Define hyperparameters for the solver\n",
        "data = {\n",
        "    'X_train': cp.asarray(data[\"X_train\"]),\n",
        "    'y_train': cp.asarray(data[\"y_train\"]),\n",
        "    'X_val': cp.asarray(data[\"X_val\"]),\n",
        "    'y_val': cp.asarray(data[\"y_val\"])\n",
        "}\n",
        "\n",
        "solver = Solver(\n",
        "    model=model,\n",
        "    data=data,\n",
        "    update_rule='sgd',\n",
        "    optim_config={\n",
        "        'learning_rate': 1e-2 * 1.2,\n",
        "    },\n",
        "    lr_decay=0.95,\n",
        "    num_epochs=20,\n",
        "    batch_size=100,\n",
        "    print_every=100,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "solver.train()\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e842c7e9-3ad8-48b8-88ec-48835baba335",
      "metadata": {
        "id": "e842c7e9-3ad8-48b8-88ec-48835baba335"
      },
      "source": [
        "### Plot to visualize training loss and train / val accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b601859-11b8-447e-81d0-0433b1d75516",
      "metadata": {
        "id": "6b601859-11b8-447e-81d0-0433b1d75516"
      },
      "outputs": [],
      "source": [
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Training loss')\n",
        "plt.plot(solver.loss_history.get(), 'o')\n",
        "plt.xlabel('Iteration')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(solver.train_acc_history, '-o', label='train')\n",
        "plt.plot(solver.val_acc_history, '-o', label='val')\n",
        "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='lower right')\n",
        "plt.gcf().set_size_inches(15, 12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef09f646-dab7-4ce9-8abd-ce2c57a04669",
      "metadata": {
        "id": "ef09f646-dab7-4ce9-8abd-ce2c57a04669"
      },
      "source": [
        "# Part #3: Hyper-parameter tuning\n",
        "- **Your task #16**. Tune hyperparameters using the validation set. Store your best trained model in `best_model`. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.\n",
        "\n",
        "- **Minimum Goal**. You should be aim to achieve a classification accuracy of greater than 45% on the validation set. Your goal in this exercise is to get as good of a result on CIFAR-10 as you can (52% could serve as a reference), with a fully-connected Neural Network. Feel free implement your own techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "939a7bd4-044d-4395-b73a-e5c8689dddec",
      "metadata": {
        "id": "939a7bd4-044d-4395-b73a-e5c8689dddec"
      },
      "outputs": [],
      "source": [
        "best_model = None\n",
        "\n",
        "# Your task #16\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "results = {}\n",
        "best_val = -1\n",
        "\n",
        "# model pre\n",
        "input_size = 32 * 32 * 3 + 1 # 1 for bias\n",
        "hidden_size = 50\n",
        "num_classes = 10\n",
        "\n",
        "# Provided as a reference. You may or may not want to change these hyperparameters\n",
        "learning_rates = np.linspace(1e-2, 1e-1, 10)\n",
        "lr_decays = np.linspace(0.995, 0.9995, 10)\n",
        "\n",
        "# Your task #5\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# Iterate over all combinations of learning rates and regularization strengths\n",
        "X_train = cp.asarray(data[\"X_train\"])\n",
        "y_train = cp.asarray(data[\"y_train\"])\n",
        "X_val = cp.asarray(data[\"X_val\"])\n",
        "y_val = cp.asarray(data[\"y_val\"])\n",
        "for lr in learning_rates:\n",
        "    for lr_decay in lr_decays:\n",
        "###########################\n",
        "        print( \\\n",
        "            f\"Training with hidden_size={hidden_size}, learning_rate={lr}, \" \\\n",
        "            f\"lr_decay={lr_decay}, num_epochs={20}\" \\\n",
        "        )\n",
        "\n",
        "\n",
        "        # Initialize the model\n",
        "        model = TwoLayerNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "        # Initialize the solver\n",
        "        solver = Solver(\n",
        "            model=model,\n",
        "            data=data,\n",
        "            update_rule='sgd',\n",
        "            optim_config={\n",
        "                'learning_rate': lr,\n",
        "            },\n",
        "            lr_decay=lr_decay,\n",
        "            num_epochs=20,\n",
        "            batch_size=100,\n",
        "            print_every=100,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        solver.train()\n",
        "\n",
        "        # Check validation accuracy\n",
        "        val_acc = solver.best_val_acc\n",
        "        print(f\"Validation accuracy: {val_acc}\")\n",
        "\n",
        "        # Update the best model if validation accuracy improves\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_model = model\n",
        "            print(\"New best model found!\")\n",
        "\n",
        "    print(f\"Best validation accuracy achieved: {best_val}\")\n",
        "###########################\n",
        "\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9df15903-8076-46a1-b36a-95789df9849a",
      "metadata": {
        "id": "9df15903-8076-46a1-b36a-95789df9849a"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "855a90b0-7fb6-4c22-86de-bed34b7e9bbb",
      "metadata": {
        "id": "855a90b0-7fb6-4c22-86de-bed34b7e9bbb"
      },
      "outputs": [],
      "source": [
        "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
        "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcf51391-d824-4601-96db-e8ba93e14199",
      "metadata": {
        "id": "dcf51391-d824-4601-96db-e8ba93e14199"
      },
      "outputs": [],
      "source": [
        "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
        "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "maver",
      "language": "python",
      "name": "naver"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}